{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when creating and registering allocator: An allocator for this device has already been registered for sharing.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize with local model path. Can also read .mf.gz files, but we recommend decompressing\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# up-front to avoid decompression overhead every time the model is initialized.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/dotronghiep/Documents/Research/VLM_Robot/moondream/moondream-0_5b-int8.mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load and process image\u001b[39;00m\n\u001b[32m      9\u001b[39m image = Image.open(\u001b[33m\"\u001b[39m\u001b[33m/home/dotronghiep/Documents/Research/VLM_Robot/0002_c1s1_069056_02.jpg\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/reid/lib/python3.12/site-packages/moondream/__init__.py:12\u001b[39m, in \u001b[36mvl\u001b[39m\u001b[34m(model, api_key)\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CloudVL(api_key)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOnnxVL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEither model_path or api_key must be provided.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/reid/lib/python3.12/site-packages/moondream/onnx_vl.py:106\u001b[39m, in \u001b[36mOnnxVL.from_path\u001b[39m\u001b[34m(cls, model_path)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# Fall back to CPU if no GPU is available.\u001b[39;00m\n\u001b[32m    100\u001b[39m     ort_memory_info = ort.OrtMemoryInfo(\n\u001b[32m    101\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    102\u001b[39m         ort.OrtAllocatorType.ORT_ARENA_ALLOCATOR,\n\u001b[32m    103\u001b[39m         \u001b[32m0\u001b[39m,\n\u001b[32m    104\u001b[39m         ort.OrtMemType.DEFAULT,\n\u001b[32m    105\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mort\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_and_register_allocator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mort_memory_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     sess_options = ort.SessionOptions()\n\u001b[32m    108\u001b[39m     sess_options.enable_cpu_mem_arena = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Error when creating and registering allocator: An allocator for this device has already been registered for sharing."
     ]
    }
   ],
   "source": [
    "import moondream as md\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize with local model path. Can also read .mf.gz files, but we recommend decompressing\n",
    "# up-front to avoid decompression overhead every time the model is initialized.\n",
    "model = md.vl(model=\"/home/dotronghiep/Documents/Research/VLM_Robot/moondream/moondream-0_5b-int8.mf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process image\n",
    "image = Image.open(\"/home/dotronghiep/Documents/Research/VLM_Robot/0002_c1s1_069056_02.jpg\")\n",
    "encoded_image = model.encode_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption:  The image shows a person standing on a paved surface, possibly a sidewalk or street. The person is wearing a black backpack and a red shirt. Their face is partially obscured by a black backpack. The person's hair is dark and appears to be styled in a short, possibly wavy, cut. The background is blurred, suggesting a shallow depth of field. The image is in color, with a predominantly dark and muted color palette. The person's clothing and the background are the main focus of the image.\n",
      "Answer:  The person in the photo is identified by the name \"Lily\" and the name \"Lily\" on her back.\n",
      "Answer:  The person in the photo is a young woman wearing a red shirt and blue shorts. She is walking down a sidewalk, and her hair is pulled back into a ponytail.\n",
      "Answer:  The person in the image is a young woman with short dark hair. She is wearing a red shirt and blue shorts.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Generate caption\n",
    "caption = model.caption(encoded_image)[\"caption\"]\n",
    "print(\"Caption:\", caption)\n",
    "\n",
    "# Ask questions\n",
    "answer = model.query(encoded_image, \"Briefly describe the identifying features of the person in the photo\")[\"answer\"]\n",
    "print(\"Answer:\", answer)\n",
    "# Ask questions\n",
    "answer = model.query(encoded_image, \"Briefly describe the person in the photo.\")[\"answer\"]\n",
    "print(\"Answer:\", answer)\n",
    "# Ask questions\n",
    "answer = model.query(encoded_image, \"Describe the person in the image, including clothing, body shape, estimated age, and gender.\")[\"answer\"]\n",
    "print(\"Answer:\", answer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
